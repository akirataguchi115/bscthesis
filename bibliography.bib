@misc{boucher2021bad,
  title         = {Bad Characters: Imperceptible NLP Attacks},
  author        = {Nicholas Boucher and Ilia Shumailov and Ross Anderson and Nicolas Papernot},
  year          = {2021},
  eprint        = {2106.09898},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{yang-etal-2021-rethinking,
  title     = {Rethinking Stealthiness of Backdoor Attack against {NLP} Models},
  author    = {Yang, Wenkai  and
               Lin, Yankai  and
               Li, Peng  and
               Zhou, Jie  and
               Sun, Xu},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.acl-long.431},
  doi       = {10.18653/v1/2021.acl-long.431},
  pages     = {5543--5557},
  abstract  = {Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at https://github.com/lancopku/SOS.}
}

@article{DBLP:journals/corr/abs-2111-07970,
  author     = {Leilei Gan and
                Jiwei Li and
                Tianwei Zhang and
                Xiaoya Li and
                Yuxian Meng and
                Fei Wu and
                Shangwei Guo and
                Chun Fan},
  title      = {Triggerless Backdoor Attack for {NLP} Tasks with Clean Labels},
  journal    = {CoRR},
  volume     = {abs/2111.07970},
  year       = {2021},
  url        = {https://arxiv.org/abs/2111.07970},
  eprinttype = {arXiv},
  eprint     = {2111.07970},
  timestamp  = {Tue, 16 Nov 2021 12:12:31 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2111-07970.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{https://doi.org/10.1002/aris.1440370103,
  author  = {Chowdhury, Gobinda G.},
  title   = {Natural language processing},
  journal = {Annual Review of Information Science and Technology},
  volume  = {37},
  number  = {1},
  pages   = {51-89},
  doi     = {https://doi.org/10.1002/aris.1440370103},
  url     = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/aris.1440370103},
  eprint  = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/aris.1440370103},
  year    = {2003}
}

@article{doi:10.1126/science.aaa8685,
  author   = {Julia Hirschberg  and Christopher D. Manning },
  title    = {Advances in natural language processing},
  journal  = {Science},
  volume   = {349},
  number   = {6245},
  pages    = {261-266},
  year     = {2015},
  doi      = {10.1126/science.aaa8685},
  url      = {https://www.science.org/doi/abs/10.1126/science.aaa8685},
  eprint   = {https://www.science.org/doi/pdf/10.1126/science.aaa8685},
  abstract = {Natural language processing employs computational techniques for the purpose of learning, understanding, and producing human language content. Early computational approaches to language research focused on automating the analysis of the linguistic structure of language and developing basic technologies such as machine translation, speech recognition, and speech synthesis. Today’s researchers refine and make use of such tools in real-world applications, creating spoken dialogue systems and speech-to-speech translation engines, mining social media for information about health or finance, and identifying sentiment and emotion toward products and services. We describe successes and challenges in this rapidly advancing area.}
}

@misc{moore1965cramming,
  title     = {Cramming more components onto integrated circuits},
  author    = {Moore, Gordon E and others},
  year      = {1965},
  publisher = {McGraw-Hill New York}
}

@article{jordan2015machine,
  title     = {Machine learning: Trends, perspectives, and prospects},
  author    = {Jordan, Michael I and Mitchell, Tom M},
  journal   = {Science},
  volume    = {349},
  number    = {6245},
  pages     = {255--260},
  year      = {2015},
  publisher = {American Association for the Advancement of Science}
}

@article{gopalakrishnan2018deep,
  title     = {Deep learning in data-driven pavement image analysis and automated distress detection: A review},
  author    = {Gopalakrishnan, Kasthurirangan},
  journal   = {Data},
  volume    = {3},
  number    = {3},
  pages     = {28},
  year      = {2018},
  publisher = {Multidisciplinary Digital Publishing Institute}
}

@misc{https://doi.org/10.48550/arxiv.1911.07399,
  doi       = {10.48550/ARXIV.1911.07399},
  url       = {https://arxiv.org/abs/1911.07399},
  author    = {Huang, Xijie and Alzantot, Moustafa and Srivastava, Mani},
  keywords  = {Cryptography and Security (cs.CR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {NeuronInspect: Detecting Backdoors in Neural Networks via Output Explanations},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{adversexamples,
  author = {Aleksander Mądry and Ludwig Schmidt },
  title  = {A Brief Introduction to Adversarial Examples},
  year   = {2018},
  doi    = {10.1126/science.aaa8685},
  url    = {https://gradientscience.org/intro_adversarial/}
}

@inproceedings{spamfilter,
  author    = {Kuchipudi, Bhargav and Nannapaneni, Ravi Teja and Liao, Qi},
  title     = {Adversarial Machine Learning for Spam Filters},
  year      = {2020},
  isbn      = {9781450388337},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3407023.3407079},
  doi       = {10.1145/3407023.3407079},
  abstract  = {Email spam filters based on machine learning techniques are widely deployed in today's organizations. As our society relies more on artificial intelligence (AI), the security of AI, especially the machine learning algorithms, becomes increasingly important and remains largely untested. Adversarial machine learning, on the other hand, attempts to defeat machine learning models through malicious input. In this paper, we experiment how adversarial scenario may impact the security of machine learning based mechanisms such as email spam filters. Using natural language processing (NLP) and Baysian model as an example, we developed and tested three invasive techniques, i.e., synonym replacement, ham word injection and spam word spacing. Our adversarial examples and results suggest that these techniques are effective in fooling the machine learning models. The study calls for more research on understanding and safeguarding machine learning based security mechanisms in the presence of adversaries.},
  booktitle = {Proceedings of the 15th International Conference on Availability, Reliability and Security},
  articleno = {38},
  numpages  = {6},
  keywords  = {adversarial machine learning, network security, spam detection, artificial intelligence},
  location  = {Virtual Event, Ireland},
  series    = {ARES '20}
}